{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a54ea15d",
      "metadata": {
        "id": "a54ea15d"
      },
      "source": [
        "## Model Comparison and Analysis\n",
        "\n",
        "### Training Process\n",
        "- **Original Model**: Training ceased once the loss fell below 0.4. This was achieved in 20 epochs, with a final training loss of approximately 0.393.\n",
        "- **Modified Model**: Training stopped at a more stringent loss threshold of 0.2, taking only 14 epochs to reach this target. The final training loss recorded was about 0.196.\n",
        "\n",
        "### Performance Outcomes\n",
        "- **Original Model Test Accuracy**: Achieved 84.21% on the 10,000 image test set.\n",
        "- **Modified Model Test Accuracy**: Improved to 87.25% on the same test set.\n",
        "\n",
        "#### Epochs and Training Loss\n",
        "- The modified model required fewer epochs to reach a lower loss threshold, indicating an efficient learning process due to the implemented changes in the model architecture and training regimen.\n",
        "\n",
        "#### Test Accuracy\n",
        "- There was a notable increase in test accuracy in the modified model, suggesting better generalization and feature representation learned during training.\n",
        "\n",
        "#### Model Architecture Changes\n",
        "- The modified model included additional pooling layers (based on assignment details), potentially enhancing feature extraction and reducing overfitting, leading to improved test accuracy.\n",
        "\n",
        "#### Loss Function and Optimizer\n",
        "- The original model used SGD and NLLLoss, while the modified model switched to Adam optimizer and CrossEntropyLoss. This change likely contributed significantly to the faster convergence and increased test accuracy due to Adam's efficiency and CrossEntropyLoss's numerical stability.\n",
        "\n",
        "#### Faster Convergence with Adam\n",
        "- The Adam optimizer, known for adjusting learning rates adaptively, likely led to quicker and more stable convergence compared to SGD.\n",
        "\n",
        "#### Stringent Loss Threshold and Efficient Learning\n",
        "- Training the modified model to a lower loss value without overfitting, as evidenced by the higher test accuracy, indicates a better fitting model.\n",
        "\n",
        "#### Potential Impact of Pooling Layers\n",
        "- Pooling layers aided in creating a more robust representation of features, which is beneficial for classification tasks.\n",
        "\n",
        "#### Learning Rate and Training Epochs\n",
        "- The modified model not only achieved a lower loss threshold faster but also improved test accuracy, underscoring the efficiency of the learning process with the new architecture and training parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36068e17",
      "metadata": {
        "id": "36068e17"
      },
      "source": [
        "## Extended Model Analysis with Lower Loss Threshold\n",
        "\n",
        "### Training Process Adjustment\n",
        "- The loss threshold was adjusted to a target of `0.1`.\n",
        "- The training loop incorporated a check to save the model with the lowest loss observed and to break if the loss no longer improved significantly (`< 0.001` difference).\n",
        "\n",
        "### Results\n",
        "- **Final Training Loss**: Achieved a loss of `0.109`, indicating that the model was able to train to a more precise fitting than the previous threshold of `0.2`.\n",
        "- **Training Completion**: The model training completed successfully, indicating that the lower loss threshold was attainable.\n",
        "- **Test Accuracy**: The model achieved `87.64%` accuracy on the test dataset.\n",
        "\n",
        "### Analysis\n",
        "#### Training Efficiency\n",
        "- Despite the lower threshold, the model successfully achieved a lower training loss without requiring an excessive number of additional epochs, showcasing the effectiveness of the model's capacity and the chosen optimizer.\n",
        "\n",
        "#### Model Generalization\n",
        "- A slight increase in test accuracy was observed compared to the previous model (`87.25%` to `87.64%`). This suggests that the model did not overfit despite being trained to a lower loss threshold, maintaining its generalization capabilities on unseen data.\n",
        "\n",
        "Implementing a model saving strategy based on the lowest loss observed ensures that the best-performing model state is retained, regardless of subsequent epochs, which is a good practice for avoiding overfitting.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}